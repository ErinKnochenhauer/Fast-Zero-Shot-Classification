{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Text Classification -- when you have no labeled data\n",
    "\n",
    "This notebook demonstrates the process of training an efficient student classifier based off predictions (labeled data) from a pretrained Hugging Face Zero Shot classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    TextClassificationPipeline,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utilities.distill_classifier_ import (\n",
    "    ZeroShotStudentTrainer,\n",
    "    read_lines,\n",
    "    get_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"distilled_text_classifier\"\n",
    "\n",
    "# read in synthetic data\n",
    "EXAMPLES = read_lines('./examples.txt')\n",
    "\n",
    "# define example class names\n",
    "CLASS_NAMES = [\n",
    "    'quality',\n",
    "    'texture',\n",
    "    'scent',\n",
    "    'value',\n",
    "    'results',\n",
    "    'color',\n",
    "    'dryness',\n",
    "    'brightening',\n",
    "    'staining',\n",
    "    'experience',\n",
    "    'quantity',\n",
    "    'longevity',\n",
    "    'antiaging'\n",
    "]\n",
    "\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    seed=48,\n",
    "    fp16=False,\n",
    "    local_rank=-1\n",
    ")\n",
    "\n",
    "# TOKENIZERS_PARALLELISM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = EXAMPLES[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize zero shot student trainer with chosen text and class names\n",
    "zero_shot_student_trainer = ZeroShotStudentTrainer(train,\n",
    "                                                   class_names=CLASS_NAMES,\n",
    "                                                   hypothesis_template=\"This text is about {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions from zero-shot teacher model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8216894ca664325af678bcd94dfe419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing student model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into training and testing\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "Tokenizing training and testing datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ee96ebd6e048328cc37d4e20d2d9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c27f4720fd42acaf7bf1421e467a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "Training student model on teacher predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erinknochenhauer/repos/text-classification/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ade18172dd411dbc2975a9d9293706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9f73bc2d30485c91303ebe20a6e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6830998063087463, 'eval_agreement': 0.0, 'eval_runtime': 0.1831, 'eval_samples_per_second': 5.462, 'eval_steps_per_second': 5.462, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7763f8e7028947c6b9ac77f8ea3beba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6665518879890442, 'eval_agreement': 0.0, 'eval_runtime': 0.126, 'eval_samples_per_second': 7.935, 'eval_steps_per_second': 7.935, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2227a350c42d4a11b6dd9cc607b5a660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6522639393806458, 'eval_agreement': 0.0, 'eval_runtime': 0.148, 'eval_samples_per_second': 6.759, 'eval_steps_per_second': 6.759, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4c242bef20453e89c6b65b97871bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6446555852890015, 'eval_agreement': 0.0, 'eval_runtime': 0.2043, 'eval_samples_per_second': 4.894, 'eval_steps_per_second': 4.894, 'epoch': 4.0}\n",
      "{'train_runtime': 27.9975, 'train_samples_per_second': 1.286, 'train_steps_per_second': 0.286, 'train_loss': 0.6709115505218506, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e454129bb6947b29cbf9ee4d03e5559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement of student and teacher predictions: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from Teacher model and train the student model based off these predictions\n",
    "zero_shot_student_trainer.distill_text_classifier(TRAINING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and student model\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erinknochenhauer/repos/text-classification/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "student_distilled_pipeline = TextClassificationPipeline(model=model,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        return_all_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'texture', 'score': 0.4992130398750305}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_distilled_pipeline(\"slightly thick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Speed between Original and Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = EXAMPLES[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test original Zero Shot Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56a024021e148d18793998544dc5fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  47.40 seconds\n"
     ]
    }
   ],
   "source": [
    "zero_shot_classifier = pipeline('zero-shot-classification', model=\"roberta-large-mnli\")\n",
    "\n",
    "start = time()\n",
    "batch_size = 32\n",
    "hypothesis_template = \"This text is about {}.\"\n",
    "preds = []\n",
    "for i in tqdm(range(0, len(test), batch_size)):\n",
    "    examples = test[i:i+batch_size]\n",
    "    outputs = zero_shot_classifier(examples, CLASS_NAMES, hypothesis_template=hypothesis_template)\n",
    "    preds += [CLASS_NAMES.index(o['labels'][0]) for o in outputs]\n",
    "\n",
    "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58855b8871834a6fb86b94d0cb2a672f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  2.13 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "batch_size = 128  # larger batch size bc distilled model is more memory efficient\n",
    "preds = []\n",
    "for i in tqdm(range(0, len(test), batch_size)):\n",
    "    examples = test[i:i+batch_size]\n",
    "    outputs = student_distilled_pipeline(examples)\n",
    "    preds += [CLASS_NAMES.index(o['label']) for o in outputs]\n",
    "\n",
    "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>less wasteful tube</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heel within</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>complexion matte</td>\n",
       "      <td>staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slightly more flat</td>\n",
       "      <td>staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed star</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>real deal guy</td>\n",
       "      <td>staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trump manual</td>\n",
       "      <td>staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>four stars good eyeliner</td>\n",
       "      <td>staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mauve mama</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>happen exfoliate</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text     label\n",
       "0        less wasteful tube     value\n",
       "1               heel within     value\n",
       "2          complexion matte  staining\n",
       "3        slightly more flat  staining\n",
       "4         disappointed star   quality\n",
       "5             real deal guy  staining\n",
       "6              trump manual  staining\n",
       "7  four stars good eyeliner  staining\n",
       "8                mauve mama   quality\n",
       "9          happen exfoliate     value"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results_df(test, class_names=CLASS_NAMES, preds=preds)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
