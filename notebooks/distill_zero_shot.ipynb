{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Text Classification -- when you have no labeled data\n",
    "\n",
    "This notebook demonstrates the process of training an efficient student classifier based off predictions (labeled data) from a pretrained Hugging Face Zero Shot classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    TextClassificationPipeline,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utilities.distill_classifier_ import (\n",
    "    ZeroShotStudentTrainer,\n",
    "    read_lines,\n",
    "    get_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../distilled_text_classifier\"\n",
    "\n",
    "# read in synthetic data\n",
    "EXAMPLES = read_lines('./examples.txt')\n",
    "\n",
    "# define example class names\n",
    "CLASS_NAMES = [\n",
    "    'quality',\n",
    "    'texture',\n",
    "    'scent',\n",
    "    'value',\n",
    "    'results',\n",
    "    'color',\n",
    "    'dryness',\n",
    "    'brightening',\n",
    "    'staining',\n",
    "    'experience',\n",
    "    'quantity',\n",
    "    'longevity',\n",
    "    'antiaging'\n",
    "]\n",
    "\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    seed=48,\n",
    "    fp16=False,\n",
    "    local_rank=-1\n",
    ")\n",
    "\n",
    "# TOKENIZERS_PARALLELISM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = EXAMPLES[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize zero shot student trainer with chosen text and class names\n",
    "zero_shot_student_trainer = ZeroShotStudentTrainer(train,\n",
    "                                                   class_names=CLASS_NAMES,\n",
    "                                                   hypothesis_template=\"This text is about {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions from zero-shot teacher model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e34a73279dd4164a36953bf99960cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing student model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into training and testing\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Tokenizing training and testing datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dfad3a6dfb4e9781d587cce3963b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d466b5e62a43424bade7603a4fcf43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Training student model on teacher predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erinknochenhauer/repos/text-classification/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff78cadcf2b04a10b4ae5f202338ea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2742, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2544, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99001f0f3724dd8940a83e45c14724b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2511460483074188, 'eval_agreement': 0.643, 'eval_runtime': 9.748, 'eval_samples_per_second': 102.585, 'eval_steps_per_second': 12.823, 'epoch': 1.0}\n",
      "{'loss': 0.2511, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2497, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a90c2df89924a7380962c4c618258db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24967394769191742, 'eval_agreement': 0.669, 'eval_runtime': 9.5117, 'eval_samples_per_second': 105.134, 'eval_steps_per_second': 13.142, 'epoch': 2.0}\n",
      "{'loss': 0.2489, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}\n",
      "{'loss': 0.2468, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a578ed24ac6c45efbf43368cd33d09f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24932238459587097, 'eval_agreement': 0.67, 'eval_runtime': 9.4953, 'eval_samples_per_second': 105.315, 'eval_steps_per_second': 13.164, 'epoch': 3.0}\n",
      "{'loss': 0.2472, 'learning_rate': 1.1111111111111112e-05, 'epoch': 3.11}\n",
      "{'loss': 0.2459, 'learning_rate': 5.555555555555556e-06, 'epoch': 3.56}\n",
      "{'loss': 0.2463, 'learning_rate': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9d2928345c4c2f9be521b7a953748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24907821416854858, 'eval_agreement': 0.691, 'eval_runtime': 10.0169, 'eval_samples_per_second': 99.831, 'eval_steps_per_second': 12.479, 'epoch': 4.0}\n",
      "{'train_runtime': 3548.5545, 'train_samples_per_second': 10.145, 'train_steps_per_second': 1.268, 'train_loss': 0.2516319105360243, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72e85bf82684e8b9396c69f378236d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement of student and teacher predictions: 69.10%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from Teacher model and train the student model based off these predictions\n",
    "zero_shot_student_trainer.distill_text_classifier(TRAINING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and student model\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erinknochenhauer/repos/text-classification/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "student_distilled_pipeline = TextClassificationPipeline(model=model,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        return_all_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'texture', 'score': 0.25389814376831055}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_distilled_pipeline(\"slightly thick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Speed between Original and Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = EXAMPLES[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test original Zero Shot Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c052964b2e0427388e80ba4279e133f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  4106.51 seconds\n"
     ]
    }
   ],
   "source": [
    "zero_shot_classifier = pipeline('zero-shot-classification', model=\"roberta-large-mnli\")\n",
    "\n",
    "start = time()\n",
    "batch_size = 32\n",
    "hypothesis_template = \"This text is about {}.\"\n",
    "preds = []\n",
    "for i in tqdm(range(0, len(test), batch_size)):\n",
    "    examples = test[i:i+batch_size]\n",
    "    outputs = zero_shot_classifier(examples, CLASS_NAMES, hypothesis_template=hypothesis_template)\n",
    "    preds += [CLASS_NAMES.index(o['labels'][0]) for o in outputs]\n",
    "\n",
    "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71edbf495c934ea9a78a978bf8c7b60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  31.71 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "batch_size = 128  # larger batch size bc distilled model is more memory efficient\n",
    "preds = []\n",
    "for i in tqdm(range(0, len(test), batch_size)):\n",
    "    examples = test[i:i+batch_size]\n",
    "    outputs = student_distilled_pipeline(examples)\n",
    "    preds += [CLASS_NAMES.index(o['label']) for o in outputs]\n",
    "\n",
    "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>religious on</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>different reasonsoccasion</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strechmark recovery</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need caffeine everyday</td>\n",
       "      <td>quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home bleach</td>\n",
       "      <td>color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>real deal guy</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>trump manual</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>four stars good eyeliner</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>mauve mama</td>\n",
       "      <td>scent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>happen exfoliate</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text       label\n",
       "0                 religious on  experience\n",
       "1    different reasonsoccasion  experience\n",
       "2          strechmark recovery     results\n",
       "3       need caffeine everyday    quantity\n",
       "4                  home bleach       color\n",
       "..                         ...         ...\n",
       "995              real deal guy       value\n",
       "996               trump manual  experience\n",
       "997   four stars good eyeliner     quality\n",
       "998                 mauve mama       scent\n",
       "999           happen exfoliate     results\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results_df(test, class_names=CLASS_NAMES, preds=preds)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
